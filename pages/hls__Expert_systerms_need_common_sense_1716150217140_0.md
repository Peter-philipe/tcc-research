file:: [Expert_systerms_need_common_sense_1716150217140_0.pdf](../assets/Expert_systerms_need_common_sense_1716150217140_0.pdf)
file-path:: ../assets/Expert_systerms_need_common_sense_1716150217140_0.pdf

- While the dialog is in English, MYCIN avoids having to understand freely written English by controlling the dialog.
  ls-type:: annotation
  hl-page:: 2
  hl-color:: green
  id:: 664bf3e0-9be7-4bfe-a564-d9728c205bfd
- It would therefore be diﬃcult to modify MYCIN to learn from its experience.
  ls-type:: annotation
  hl-page:: 2
  hl-color:: red
  id:: 664bf8c3-3482-47ac-8dc2-53f40d62048b
- MYCIN’s ontology includes bacteria, symptoms, tests, possible sites of infection, antibiotics and treatments. Doctors, hospitals, illness and death are absent. Even patients are not really part of the ontology, although MYCIN asks for many facts about the speciﬁc patient. This is because patients aren’t values of variables, and MYCIN never compares the infections of two diﬀerent patients
  ls-type:: annotation
  hl-page:: 2
  hl-color:: green
  id:: 664c0192-e963-4739-9532-d542749e07b1
- for a program to plan intelligently, it must be able to determine the eﬀects of its own actions.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: blue
  id:: 664c0acb-7411-46d6-8594-bc44de09957e
- but MYCIN makes no prognosis of the eﬀects of the treatment. Of course, the doctors who provided the information built into MYCIN considered the eﬀects of the treatments
  ls-type:: annotation
  hl-page:: 4
  hl-color:: blue
  id:: 664c0aea-c934-4d24-80bc-087b1b2ec8d7
- In other domains, expert systems and other AI programs have to make plans, but MYCIN doesn’t.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: red
  id:: 664c0d5e-3cf5-42a0-8030-d79956ab6204
- Moreover, MYCIN cannot answer a question about a hypothetical treatment, e.g.“What will happen if I give this patient penicillin?” or even “What bad things might happen if I give this patient penicillin?”.
  ls-type:: annotation
  hl-page:: 5
  hl-color:: red
  id:: 6652299f-b594-4ea7-9c46-c7d0506e7eaf
- An example of MYCIN not knowing its limitations can be excited by telling MYCIN that the patient has Cholerae Vibrio in his intestines. MYCIN will cheerfully recommend two weeks of tetracycline and nothing else. Presumably this would indeed kill the bacteria, but most likely the patient will be dead of cholera long before that. However, the physician will presumably know that the diarrhea has to be treated and look elsewhere for how to do it.
  ls-type:: annotation
  hl-page:: 3
  hl-color:: green
  id:: 678d5a56-8f37-42a6-8cc4-6ba22906d63f
- Various formalisms are used in artiﬁcial intelligence for representing facts about the eﬀects of actions and other events. However, all systems that I know about give the eﬀects of an event in a situation by describing a new situation that results from the event. This is often enough, but it doesn’t cover the important case of concurrent events and actions. For example, if a patient has cholera, while the antibiotic is killing the cholera bacteria, the damage to his intestines is causing loss of ﬂuids that are likely to be fatal. Inventing a formalism that will conveniently express people’s common sense knowledge about concurrent events is a major unsolved problem of AI.
  ls-type:: annotation
  hl-page:: 5
  hl-color:: green
  id:: 678d5ac2-4307-471f-b055-b4e56b75fb01
- epistemologically
  ls-type:: annotation
  hl-page:: 5
  hl-color:: purple
  id:: 679c170b-404d-4188-b6f8-6d261811fac1
- It outputs sentences, but the user types only single words or standard phrases
  ls-type:: annotation
  hl-page:: 2
  hl-color:: blue
  id:: 679c237b-e3d3-43a7-8478-6be0d3b54810