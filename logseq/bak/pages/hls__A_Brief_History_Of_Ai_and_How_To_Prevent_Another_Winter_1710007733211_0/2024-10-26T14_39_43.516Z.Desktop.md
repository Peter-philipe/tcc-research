file:: [A_Brief_History_Of_Ai_and_How_To_Prevent_Another_Winter_1710007733211_0.pdf](../assets/A_Brief_History_Of_Ai_and_How_To_Prevent_Another_Winter_1710007733211_0.pdf)
file-path:: ../assets/A_Brief_History_Of_Ai_and_How_To_Prevent_Another_Winter_1710007733211_0.pdf

- ata from Nor
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 65eca5d0-eac7-44d6-9e51-fdcd0debc861
- First, many early AI systems pursued the “thinking humanly” approach to solve the problems
  ls-type:: annotation
  hl-page:: 8
  hl-color:: blue
  id:: 65ecaafc-fd99-47b9-8aef-f5150823ddab
- In other words, instead of taking a bottom-up approach starting from thoroughly analyzing the task, providing a possible solution, and turning it into an implementable algorithm, they took the opposite direction, merely relying on replicating the way humans perform the task.
  ls-type:: annotation
  hl-page:: 8
  hl-color:: green
  id:: 65ecab1e-6fe4-4e16-a4cd-ef95ac593835
- Second, there was a failure to recognize the complexity of many of the problems. Resulting from the oversimpliﬁcation of the AI frameworks proposed by Marvin Minsky, most early problem solving systems succeeded mainly on toy (simplistic) problems, by combining simple steps to come up with a solution
  ls-type:: annotation
  hl-page:: 8
  hl-color:: blue
  id:: 65ecab64-40f3-4628-90e2-152566c89411
- he third factor was related to the negative conceptions about neural networks and the limitations of their fundamental structures
  ls-type:: annotation
  hl-page:: 8
  hl-color:: blue
  id:: 65ecadeb-66e8-4266-a8c2-06d43baa03e8
- The MYCIN system was one of the successful consequent results of the new wave, developed in the mid 1970s for the purpose of blood infection diagnosis by Edward Shortliffe under the supervision of Bruce Buchanan and Stanley Cohen
  ls-type:: annotation
  hl-page:: 9
  hl-color:: green
  id:: 65ecb2f2-3399-455a-b389-0debc02d4194
- MYCIN could perform identiﬁcation of bacteria causing sepsis, and recommend antibiotics dosage based on patient weight. It could perform diagnosis on par with the human experts in the ﬁeld, and signiﬁcantly better than medical interns, beneﬁting from around 600 deduced rules in the form of a knowledge base, from extensive interviews with the experts, by means of integrating uncertainty calculations
  ls-type:: annotation
  hl-page:: 9
  hl-color:: blue
  id:: 65ecb315-43c2-4284-a8be-e2bb224e24b8
- Despite all efforts and investments made during the early 1980s, many companies could not fulﬁll their ambitious promises.
  ls-type:: annotation
  hl-page:: 9
  hl-color:: red
  id:: 65ecbcd0-ba1e-4777-b825-c2ba1bdcd041
- Hardware manufacturers declined to keep up with the requirements of specialized needs of the expert systems. Hence, the thriving industry of expert systems in the early 1980s declined tremendously and inevitably collapsed by the end of the 1990s and the AI industry faced another winter that lasted until the mid 1990s
  ls-type:: annotation
  hl-page:: 9
  hl-color:: green
  id:: 65ecbed7-7a2b-4626-a9ee-11dada615cad
  hl-stamp:: 1710014174477
- This second period of so-called “winter” in the history of AI had been so harsh that AI researchers subsequently tended to avoid even the term“AI” by choosing other titles such as “informatics” or “analytics”. 
  ls-type:: annotation
  hl-page:: 9
  hl-color:: blue
  id:: 65ecc586-276a-4395-9eeb-c7bcf91d0440
- The lesson learned during the periods of AI’s winter made researchers more conservative. As a result, during the late 1980s and the1990s, the ﬁeld of AI research witnessed a major conservative shift toward more established theories like statistics-based methods
  hl-page:: 9
  ls-type:: annotation
  id:: 65ecc690-fd90-497d-a261-efada0e23414
  hl-color:: green
  collapsed:: true
- Another important outcome of this conservative shift in the ﬁeld of AI was the development of public benchmark datasets and related competitions in its various subﬁelds
  ls-type:: annotation
  hl-page:: 9
  hl-color:: green
  id:: 65ecc80d-e846-4b7e-bbf8-d735bfbb32b7
- By 2011 the computing power of graphics processing units had grown enough to help the researchers train deep networks with higher dimensions both in terms of width and depth in a shorter time. Since the earlier implementation of Convolutional Neural Networks on graphics processing units in 2006 [85 ] which resulted in four times faster performance compared with central processing units
  ls-type:: annotation
  hl-page:: 10
  hl-color:: green
  id:: 65ed174c-6cdc-444b-a34f-e67259e99f7a
  hl-stamp:: 1710036815294
- Chess-playing AI software developed in IBM, called “Deep Blue”, eventually won over the great maestro chess world champion, Garry Kasparov. Broadcasted live, Deep Blue captured the public’s imagination once again toward AI systems of the future. The news was so breathtaking that IBM’s share values rose up to all-time highs
  ls-type:: annotation
  hl-page:: 10
  hl-color:: purple
  id:: 65ed17fa-e8a5-4eff-83f9-c1ec29dedd08
- Notable steps were taken in 2011 when IBM’s Watson defeated human champions in the highly popular TV quiz show Jeopardy
  ls-type:: annotation
  hl-page:: 10
  hl-color:: purple
  id:: 65ed1802-77d8-40ea-ace2-b7a773469d49
- The progress of deep neural networks gained public attention in 2016 at the time when Deep Mind’s AlphaGo beat the world champion of Go
  ls-type:: annotation
  hl-page:: 10
  hl-color:: purple
  id:: 65ed181f-756b-4c3f-a5f8-410b0004f988
- Massive advances in microchip manufacturing technologies in the late 1990’s led to emerging powerful computers, concurrent to the growth of the global Internet that generated massive amounts of data. This information included enormous unprocessed text, video, voice, and images, along with semiprocessed data such as geographical tracking, social media-related data, and electronic medical records, ushering in the era of big data [ 80].
  ls-type:: annotation
  hl-page:: 10
  hl-color:: blue
  id:: 65ed1950-694a-45f6-86b6-781887bef6d7
- In 1989 Yann LeCun revisited convolutional neural networks, and using gradient descend in their training mechanism, demonstrated the ability to perform well in computer vision problems, speciﬁcally in handwritten digit recognition[ 83 ].
  ls-type:: annotation
  hl-page:: 10
  hl-color:: blue
  id:: 65ed1a18-e6fd-4cb4-b9b3-ccbf104f475a
- Instances include AI-related ﬁelds such as computer vision, natural language processing, medical image diagnosis [88 ], and natural language translation.
  ls-type:: annotation
  hl-page:: 10
  hl-color:: green
  id:: 65ed1b32-3686-45af-b6bf-57b27f1ba8d6
  hl-stamp:: 1710037821263
- On a global scale, AI is becoming an attractive target for investors, producing billions of dollars of proﬁt per annum. From 2010 to 2020, global investment in AI-based startup companies has steadily grown from $1.3 billion to more than $40 billion, with an average annual growth rate of nearly 50%, whereas in 2020 only, corporate investment in AI is reported to be nearly $70 billion globally [ 90 ].
  ls-type:: annotation
  hl-page:: 11
  hl-color:: blue
  id:: 65ed1c1a-e9fb-4ee4-aaac-d5810275af71
  hl-stamp:: 1710038044092
- n the academic sector, from 2000 up until 2020, the number of peer-reviewed AI articles per year has grown roughly 12 times worldwide. AI conferences have witnessed similar signiﬁcant increases in terms of the number of attendants. In 2020, NeurIPS accepted 22,000 attendees, more than 40% growth over 2018, and 10-fold more compared with 2012. Concurrently, AI has become the most popular specialization among computer science PhD students in North America, nearly three times the next
  ls-type:: annotation
  hl-page:: 11
  hl-color:: green
  id:: 65ed1c51-afa5-475d-b41d-3e98335dcb9f
  hl-stamp:: 1710038100077
- AI has appeared particularly vulnerable to overestimations coupled to technical limitations. Overall, the hype and fear that comes with reaching human-level intelligence have quickly contributed to exaggerations and public coverage that is not common in other innovative tech sectors
  ls-type:: annotation
  hl-page:: 12
  hl-color:: blue
  id:: 65ede9a9-ed5e-4b89-8626-576ef137c7f8
- Two reports brought about major halts in supporting the research: the US government report, namely ALPAC in 1966 [ 51 ], and the Lighthill report of the British government in 1973 [ 52]. 
  ls-type:: annotation
  hl-page:: 8
  hl-color:: blue
  id:: 65ee6555-9829-4c94-bb63-061a84a5babb
- To address these drawbacks, in the early 1980s, researchers decided to take a more robust approach utilizing domain-speciﬁc information for stronger reasoning but in narrower areas of expertise. The new approach, so-called“expert systems”, originated at Carnegie Mellon University, and was quickly able to ﬁnd its way to corporations.
  ls-type:: annotation
  hl-page:: 9
  hl-color:: blue
  id:: 65efad32-b48f-407b-9f72-30278c55a7aa
- Meanwhile, one of the most important moves toward deep convolutional neural networks (CNN) happened in 1980. The “neocognitron”, the ﬁrst convolutional neural networks (CNN) architecture, was proposed by Fukushima in 1980[ 55 ].
  ls-type:: annotation
  hl-page:: 9
  hl-color:: red
  id:: 65efaeeb-f044-4d00-9ada-9fa4433123b0
- R1, developed by McDermott in 1982, was the ﬁrst successful commercial expert system used in the digital equipment industry, for the conﬁguration of new computer systems’ orders [ 56]. In nearly4 years, the ﬁrm added $40 million of revenue using R1
  ls-type:: annotation
  hl-page:: 9
  hl-color:: purple
  id:: 65efafd3-d698-4973-a468-e4ca3381859e
- Based on Alan Turing’s "On Computable Numbers [ 23]," their model provided a way to abstractly describe brain functions, and demonstrated that simple elements connected in a neural network can have enormous computational power.
  ls-type:: annotation
  hl-page:: 5
  hl-color:: green
  id:: 65f99df8-ecb1-4f86-8cb2-0cc0ecc3a577
- The article received little attention until John von Neumann, Norbert Wiener, and others applied its concepts.
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 65f99e2d-dfed-4821-98bc-fd0ed0857c6f
- Based on this work, six years later, Donald Hebb proposed a simple learning rule to tune the strength of the neuron connections [ 24 ]. His learning method, namely “Hebbian learning,” [ 25 ] is considered as the inspiring model for neural networks learning.
  ls-type:: annotation
  hl-page:: 5
  hl-color:: purple
  id:: 65f9c706-5006-434c-a772-c420c81b1b0e
- The British mathematician Alan Turing published a paper in 1950 (“Computers and intelligence” [12]) in which he proposed a tool to determine the difference between a task performed by a person and a machine. This test, known as the "Turing test", consists of a series of questions to be answered.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: red
  id:: 6614b6b0-d708-4411-9830-e3fe1a022cd5
- There, the term ’Artiﬁcial Intelligence’ was coined by McCarthy. McCarthy deﬁned AI as "the science and engineering of making intelligent machines," emphasizing the parallel growth between computers and AI. The conference is sometimes referred to as the "birthplace of AI" because it coordinated and energized the ﬁeld [ 10 ], and this time is considered as the beginning of an era called “the ﬁrst summer of AI”.
  ls-type:: annotation
  hl-page:: 6
  hl-color:: blue
  id:: 66245f5d-601e-4a34-9fb7-75ff7876438d
- A turning point in AI, and speciﬁcally in neural networks, occurred in 1957 when the psychologist researcher Frank Rosenblatt (considered a father of deep learning [ 35 ]) built the Mark I Perceptron at Cornell [36 ]. He built an analog neural network with the ability to learn through trial and error.
  ls-type:: annotation
  hl-page:: 7
  hl-color:: blue
  id:: 66246622-fcc7-4a88-b7b5-7921feb2e41b
- Another important event during the early1960s is the emergence of the ﬁrst industrial robot. Named “Unimate”, the robotic arm was used on an assembly line in General Motors in 1961 for welding and other metalworks [43 ]
  ls-type:: annotation
  hl-page:: 7
  hl-color:: blue
  id:: 6624702c-b3b9-42fb-aa5c-842ba67f262b
- ELIZA, the ﬁrst chatbot in the history of AI was developed by Joseph Weizenbaum at MIT in 1966. ELIZA was designed to serve as a virtual therapist to ask questions and provide follow-ups in response to the patient[49 ]. 
  ls-type:: annotation
  hl-page:: 8
  hl-color:: green
  id:: 66247313-82de-4d87-b939-7f18869a6b22
- SHAKEY, the ﬁrst omni-purpose mobile platform robot was also developed at Stanford Research Institute in 1966 with reasoning about its surrounding environment [50].
  ls-type:: annotation
  hl-page:: 8
  hl-color:: purple
  id:: 66247373-4e07-4664-b48b-df977d64c774
- DENDRAL [53], created at Stanford by Ed Feigenbaum, Bruce Buchanan, and Joshua Lederberg in the late 1960s and early 1970s, and inferred molecular structure from mass spectrometry data, was an early success story. DENDRAL was the ﬁrst effective knowledge-intensive system, relying on a vast range of special-purpose laws, to provide expertise rather than basic knowledge
  ls-type:: annotation
  hl-page:: 9
  hl-color:: green
  id:: 6629b87f-40d3-4459-a8d1-6909feb52fed
- In Japan, the government started a 10-year plan to keep up with the new wave by investing more than$1.3 billion in intelligent systems. 
  ls-type:: annotation
  hl-page:: 9
  hl-color:: purple
  id:: 662d2dc4-acc3-45f7-866b-adff72b0b8fe
- The US government, by establishing the Microelectronics and Computer Technology Corporation in 1982, revived AI research in hardware, chip design, and software research. The same change happened in the UK as well, resulting in reassignment of funds previously cut. All these events during the 80s led to a period of“Summer” for AI. 
  ls-type:: annotation
  hl-page:: 9
  hl-color:: green
  id:: 662d2df4-82f6-4a64-b83c-ac7d6b089be3
- he AI industry thrived from billions of dollars invested in the ﬁeld, and various activities emerged from expert systems developer companies to domain-speciﬁc hardware
  <<<<<<< HEAD
  id:: 662d2e45-acb2-4b9e-b4ee-cb616dee74c6
  ls-type:: annotation
  hl-page:: 9
  hl-color:: blue
- Among these theories ﬁnding their way to the ﬁeld were hidden Markov models (HMMs) [ 60 ]. Being strictly mathematical-based and resulting from extensive training on large real-world datasets, hidden Markov models became a trustable framework for AI research, especially in handwriting recognition and speech processing, helping them to make their way back to the industry.
  ls-type:: annotation
  hl-page:: 9
  hl-color:: blue
  id:: 66592c65-16cc-4cd7-a028-5dab1fcfed7a
- Meanwhile, the availability of huge amounts of labeled data such as millions of labeled images in the ImageNet dataset helped researchers to overcome the problem of overﬁtting.
  ls-type:: annotation
  hl-page:: 10
  hl-color:: yellow
  id:: 6667a8ee-a069-46a4-843a-6b6f662e3a36
  hl-stamp:: 1718069489246
- Eventually, in 2012 Hinton’s team proposed a deep convolutional neural network architecture, named AlexNet (after the team’s leading author Alex Krizhevsky), which was able to train more layers of neurons. Using many mechanisms and techniques such as rectiﬁed linear unit activation functions and the dropout technique, AlexNet could achieve higher discriminative power in an end-to-end fashion, that is, to feed the network with merely the pure images of the dataset [87].
  ls-type:: annotation
  hl-page:: 10
  hl-color:: red
  id:: 6667a917-bf36-4b9a-bcad-68b5d5e4cf2f
  =======
  id:: 662d2e45-acb2-4b9e-b4ee-cb616dee74c6
  >>>>>>> f09a83b (Iniciando local)