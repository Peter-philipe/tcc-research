file:: [Brief-History-of-Artificial-Intelligence_1710105556985_0.pdf](../assets/Brief-History-of-Artificial-Intelligence_1710105556985_0.pdf)
file-path:: ../assets/Brief-History-of-Artificial-Intelligence_1710105556985_0.pdf

- McCulloch and Pitts in 1943. 5 Their publication describes a computer model used to learn based on a process comparable with neurons in the human brain
  ls-type:: annotation
  hl-page:: 2
  hl-color:: green
  id:: 65ee4c94-1b06-4ec6-b439-921ed32d73e8
- The model described in their publication was referred to as the MCP neuron, and it functioned by taking in Boolean inputs 
  ls-type:: annotation
  hl-page:: 2
  hl-color:: purple
  id:: 65ee4ca1-0dc9-421c-bb93-2cb78f2d1eef
- A more sophisticated version of the MCP was published in 1958 by Rosenblatt, called the perceptron. 6 The perceptron, illustrated in (Fig. 3) processed non-Boolean inputs
  ls-type:: annotation
  hl-page:: 2
  hl-color:: green
  id:: 65ee5028-f46d-49ca-ac77-63f1f76a8712
- This situation leads to unfulfilled overly ambitious expectations and an overall decline in interest in the field by investors, also known as an AI winter
  ls-type:: annotation
  hl-page:: 4
  hl-color:: blue
  id:: 65ee54ab-f5d2-4e61-96a1-93a8e7a3a337
- In 1968, Minsky and Papert 11 demonstrated the limitations of the perceptron by identifying that a Boolean function such as the XOR function that could not be modeled using a perceptron with 2 inputs and without significant user handling.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: green
  id:: 65ee5527-402d-459f-8b25-ed5b2dc25151
- The second significant publication that played a major role in initiating the first AI winter was by Lighthill 12 in 1973, providing an overview of the hype in the field and emphasis on the very small progress in the field.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: blue
  id:: 65ee583a-2427-4090-929d-2d5e37bb4c06
- However, by the mid 1980s, it was recognized that, although expert systems were capable of performing very specific tasks, these models lacked common sense, could not be used to perform more complex tasks, and were not generalizable.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: green
  id:: 65ee60f5-fccf-4740-b99a-7c03b706f0a1
- The second AI winter was due to the increased hype in the capabilities of neural networks without sufficient advancement in computing power.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: red
  id:: 65efbbc7-13d0-4dfe-a76f-6b5e60b149e6
- This trick allowed for support vector machine algorithms to capitalize on the low computational power that was available in the early 1990s
  <<<<<<< HEAD
  hl-color:: red
  id:: 65efbcca-255c-4cc0-819f-0f7492627d0a
  hl-stamp:: 1717120933689
  ls-type:: annotation
  hl-page:: 4
  =======
  hl-color:: blue
  id:: 65efbcca-255c-4cc0-819f-0f7492627d0a
  ls-type:: annotation
  hl-page:: 4
  >>>>>>> f09a83b (Iniciando local)
- The interest in AI resurged toward the mid 1990s as computational power increased and could support the development of neural networks.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: green
  id:: 65efbd5a-a269-4bd4-80f4-44924974056c
- The capabilities of AI paired with sufficient computational power was demonstrated in 1997 when IBM developed the chess playing supercomputer Deep Blue. Deep Blue defeated the chess champion Kasparov, which led to many publications and documentary films that attracted the public’s attention to the field once again
  ls-type:: annotation
  hl-page:: 4
  hl-color:: green
  id:: 65efbd75-5b53-4828-986b-ff567ab5e9a5
- During the last decade, there were 2 main advancements that enabled neural networks to progress: data storage and graphical processing units (GPU).
  ls-type:: annotation
  hl-page:: 5
  hl-color:: blue
  id:: 65efbdcd-5682-4434-876c-672e2bf730f0
- IBM had announced that it would begin to focus attention toward the health care field and revolutionize AI in health care in the upcoming years. Although there were several successful collaborations between hospitals and IBM, some collaborations resulted in failures
  <<<<<<< HEAD
  id:: 65efbf6e-9f85-461d-a3ec-2300d93f4979
  ls-type:: annotation
  hl-page:: 6
  hl-color:: blue
- The support vector machine algorithm (Figs. 4 and 5), which was originally introduced in 1963 by Vapnik and Chervonenkis, became popular again with the implementation of nonlinear kernels in 1992.20 Before Boser and colleagues’ publication in1992, 20 the support vector machine algorithm attempted to solve for a hyperplane by maximizing the marginal distances between 2 separate classes and the hyperplane. The maximized distance between the hyperplane and the classes allows for more robustness because data are always subject to noise. The publication by Boser allowed for a simple modification of the optimization algorithm, now known as the “kernel trick,” which enabled the algorithm to solve for nonlinear hyperplanes without significantly increasing the computational requirements of the algorithm
  ls-type:: annotation
  hl-page:: 4
  hl-color:: blue
  id:: 66592fac-a31c-4f16-8f29-8e1d6200c54e
- with the introduction of GPUs and the steady improvement to computers as per Moore’s law, the hardware limitations that constrained the performance of neural networks were overcome. The additional computational power enabled researchers to run larger networks with more complex layers. Cires‚ an and colleagues 25 were the first to implement GPUs with DL using a GTX 280 graphics card. Finally, in2012, Krizhevsky and colleagues 26 presented AlexNet, which used the massively available dataset from ImageNet (1.2 million images with 1000 classes at the time) with GPUs to win the ImageNet Large Scale Visual Recognition Challenge
  ls-type:: annotation
  hl-page:: 5
  hl-color:: blue
  id:: 6666169a-0ae3-482b-9cce-bb25b7dfc2b8
  =======
  id:: 65efbf6e-9f85-461d-a3ec-2300d93f4979
  >>>>>>> f09a83b (Iniciando local)